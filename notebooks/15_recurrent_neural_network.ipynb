{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPr1ECHkN0DqpLQXYhiMXHH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/machine-perception-robotics-group/JDLALectureNotebooks/blob/master/notebooks/15_recurrent_neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 回帰結合型のニューラルネットワークによる文章生成\n",
        "\n",
        "---\n",
        "## 目的\n",
        "回帰結合型のニューラルネットワーク，すなわち再帰型ニューラルネットワーク (Recurrent Neural Network; RNN) を用いてPenn Tree Bankデータセットに対する次単語の予測を行う．\n",
        "また，単方向RNN (uni-directional RNN) と双方向RNN (bidirectional RNN) の両方を用いて学習を行い，その違いを確認する．\n",
        "\n",
        "\n",
        "\n",
        "## モジュールのインポート\n",
        "プログラムの実行に必要なモジュールをインポートします．"
      ],
      "metadata": {
        "id": "Y7HxRZZpyqCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from time import time\n",
        "\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# GPUの確認\n",
        "use_cuda = torch.cuda.is_available()\n",
        "print('Use CUDA:', use_cuda)"
      ],
      "metadata": {
        "id": "eCzd2dkOeyfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## データセットの読み込み\n",
        "\n",
        "Penn Tree Bank (PTB) データセットを読み込みます．\n",
        "\n",
        "読み込んだ学習データのサイズを確認します．\n",
        "学習，検証，テストデータはそれぞれ929589，73760，82430のサイズの1次元配列になっていることがわかります．\n",
        "\n",
        "また，`get_ptb_words_vocabulary`関数を用いて，ptbデータセットに存在する英単語の情報を取得します．\n",
        "`vocab`には英単語とその単語を示すIDが辞書型のオブジェクトとして格納されています．\n",
        "英単語の数は10000です．\n",
        "\n",
        "最後に，keyと値の組み合わせを逆にした辞書`inverse_vocab`を作成します．\n",
        "これはIDで出力された予測結果から英単語を検索する際に使用します．"
      ],
      "metadata": {
        "id": "W11uPP5Nyw6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "gdown.download('https://drive.google.com/uc?id=1ceEd6c19k9GLFosyqmnfEakuxuTzWV4N', 'ptb_dataset.zip', quiet=False)\n",
        "!unzip -q -o ptb_dataset.zip"
      ],
      "metadata": {
        "id": "0t3Amf0aeymV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = np.load('ptb_dataset/ptb_train.npy')\n",
        "val = np.load('ptb_dataset/ptb_val.npy')\n",
        "test = np.load('ptb_dataset/ptb_test.npy')\n",
        "\n",
        "with open('ptb_dataset/ptb_vocab.pkl', 'rb') as f:\n",
        "    vocab = pickle.load(f)\n",
        "\n",
        "with open('ptb_dataset/ptb_inverse_vocab.pkl', 'rb') as f:\n",
        "    inverse_vocab = pickle.load(f)\n",
        "\n",
        "print(train.shape, val.shape, test.shape)\n",
        "print(len(vocab))"
      ],
      "metadata": {
        "id": "J4FLY7Hqeyo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJo8yawDOEGt"
      },
      "source": [
        "### Benn Tree Bankデータセットの表示\n",
        "\n",
        "PTBデータセットの中身を`print`関数を使って表示してみます．\n",
        "\n",
        "学習用データを表示すると，1次元配列に整数値が格納されていることがわかります．\n",
        "\n",
        "また，`vocab`のうち，英単語を指定すると，各英単語に対応するIDガ表示されます．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EL3otv4HOEGt"
      },
      "outputs": [],
      "source": [
        "print(\"train sentence:\", train)\n",
        "print(vocab['player'], vocab['primarily'], vocab['arose'], vocab['generate'], vocab['partnership'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## データセットクラスの作成\n",
        "\n",
        "PTBデータセットのためのデータセットクラスを作成します．"
      ],
      "metadata": {
        "id": "RgAP69gezcvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PTBDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, type='train', time_window=35):\n",
        "\n",
        "        if type not in ['train', 'val', 'test']:\n",
        "            raise ValueError('type must be train or val or test')\n",
        "\n",
        "        self.root = root\n",
        "        self.time_window = time_window\n",
        "\n",
        "        self.data = np.load(os.path.join(root, 'ptb_' + type + '.npy'))\n",
        "\n",
        "        with open(os.path.join(root, 'ptb_vocab.pkl'), 'rb') as f:\n",
        "            self.vocab = pickle.load(f)\n",
        "\n",
        "        with open(os.path.join(root, 'ptb_inverse_vocab.pkl'), 'rb') as f:\n",
        "            self.inverse_vocab = pickle.load(f)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index:index+self.time_window]\n",
        "        y = self.data[index+1:index+self.time_window+1]\n",
        "        return torch.from_numpy(x), torch.from_numpy(y)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.time_window - 1"
      ],
      "metadata": {
        "id": "QbsNu6Fx0aw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ネットワークモデルの定義"
      ],
      "metadata": {
        "id": "sQmi2pKA0eST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, n_vocab, n_units, bidirectional=False):\n",
        "        super(RNNLanguageModel, self).__init__()\n",
        "\n",
        "        self.n_vocab = n_vocab\n",
        "        self.n_units = n_units\n",
        "\n",
        "        self.embed = nn.Embedding(n_vocab, n_units)\n",
        "        self.rnn = nn.RNN(n_units, n_units, bidirectional=bidirectional)\n",
        "        if bidirectional:\n",
        "            self.fc = nn.Linear(n_units*2, n_vocab)\n",
        "        else:\n",
        "            self.fc = nn.Linear(n_units, n_vocab)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embed(x)\n",
        "        x, h = self.rnn(x)\n",
        "        x = self.fc(x)\n",
        "        return x, h"
      ],
      "metadata": {
        "id": "nua0WMnX0cwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 学習（単方向RNN）"
      ],
      "metadata": {
        "id": "Gw13g0Q-1eVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# パラメータの設定\n",
        "batch_size = 256\n",
        "epoch_num = 10\n",
        "\n",
        "n_vocab = len(vocab)\n",
        "n_units = 256\n",
        "\n",
        "# データセットの定義\n",
        "train_dataset = PTBDataset('ptb_dataset', type='train')\n",
        "val_dataset = PTBDataset('ptb_dataset', type='val')\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# モデルの定義（単方向RNN）\n",
        "model_unidirectional = RNNLanguageModel(n_vocab, n_units, bidirectional=False)\n",
        "if use_cuda:\n",
        "    model_unidirectional = model_unidirectional.cuda()\n",
        "\n",
        "# 損失関数と最適化アルゴリズムの設定\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "if use_cuda:\n",
        "    criterion = criterion.cuda()\n",
        "optimizer = torch.optim.Adam(model_unidirectional.parameters())\n",
        "\n",
        "model_unidirectional.train()\n",
        "\n",
        "# 学習の実行\n",
        "start = time()\n",
        "for epoch in range(1, epoch_num+1):\n",
        "\n",
        "    sum_loss = 0.0\n",
        "\n",
        "    for data, target in train_loader:\n",
        "        if use_cuda:\n",
        "            data = data.cuda()\n",
        "            target = target.cuda()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output, _ = model_unidirectional(data)\n",
        "        loss = criterion(output.view(-1, n_vocab), target.view(-1).long())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        sum_loss += loss.data\n",
        "\n",
        "    elapsed_time = time() - start\n",
        "    print('epoch: {}, loss: {:.4f}, time: {:.4f}[s]'.format(epoch, sum_loss / len(train_loader), elapsed_time))"
      ],
      "metadata": {
        "id": "PJPxK_Sy1W0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "学習したモデルで推論を行ってみます．"
      ],
      "metadata": {
        "id": "ajRzY5ZHAKpL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_unidirectional.eval()\n",
        "\n",
        "for i in range(10):\n",
        "    data, target = val_dataset[i]\n",
        "\n",
        "    if use_cuda:\n",
        "        data = data.cuda()\n",
        "\n",
        "    output, _ = model_unidirectional(data)\n",
        "    output = output.view(-1, n_vocab)\n",
        "    _, pred = torch.max(output, 1)\n",
        "\n",
        "    true_sentence = []\n",
        "    pred_sentence = []\n",
        "    for i in range(len(target.data.tolist())):\n",
        "        true_word = inverse_vocab[target.data.tolist()[i]]\n",
        "        pred_word = inverse_vocab[pred.cpu().data.tolist()[i]]\n",
        "        true_sentence.append(true_word)\n",
        "        pred_sentence.append(pred_word)\n",
        "\n",
        "    print('true:', ' '.join(true_sentence))\n",
        "    print('pred:', ' '.join(pred_sentence), '\\n')"
      ],
      "metadata": {
        "id": "IjxdINwIAKwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 学習（双方向RNN）"
      ],
      "metadata": {
        "id": "Ppc-qNDW1hhM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# パラメータの設定\n",
        "batch_size = 256\n",
        "epoch_num = 10\n",
        "\n",
        "n_vocab = len(vocab)\n",
        "n_units = 256\n",
        "\n",
        "# データセットの定義\n",
        "train_dataset = PTBDataset('ptb_dataset', type='train')\n",
        "val_dataset = PTBDataset('ptb_dataset', type='val')\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# モデルの定義（単方向RNN）\n",
        "model_bidirectional = RNNLanguageModel(n_vocab, n_units, bidirectional=True)\n",
        "if use_cuda:\n",
        "    model_bidirectional = model_bidirectional.cuda()\n",
        "\n",
        "# 損失関数と最適化アルゴリズムの設定\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "if use_cuda:\n",
        "    criterion = criterion.cuda()\n",
        "optimizer = torch.optim.Adam(model_bidirectional.parameters())\n",
        "\n",
        "model_bidirectional.train()\n",
        "\n",
        "# 学習の実行\n",
        "start = time()\n",
        "for epoch in range(1, epoch_num+1):\n",
        "\n",
        "    sum_loss = 0.0\n",
        "\n",
        "    for data, target in train_loader:\n",
        "        if use_cuda:\n",
        "            data = data.cuda()\n",
        "            target = target.cuda()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output, _ = model_bidirectional(data)\n",
        "        loss = criterion(output.view(-1, n_vocab), target.view(-1).long())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        sum_loss += loss.data\n",
        "\n",
        "    elapsed_time = time() - start\n",
        "    print('epoch: {}, loss: {:.4f}, time: {:.4f}[s]'.format(epoch, sum_loss / len(train_loader), elapsed_time))"
      ],
      "metadata": {
        "id": "vcd_XBme1hnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "学習したモデルで推論を行ってみます．"
      ],
      "metadata": {
        "id": "Vm4da9jc_qEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_bidirectional.eval()\n",
        "\n",
        "for i in range(10):\n",
        "    data, target = val_dataset[i]\n",
        "\n",
        "    if use_cuda:\n",
        "        data = data.cuda()\n",
        "\n",
        "    output, _ = model_bidirectional(data)\n",
        "    output = output.view(-1, n_vocab)\n",
        "    _, pred = torch.max(output, 1)\n",
        "\n",
        "    true_sentence = []\n",
        "    pred_sentence = []\n",
        "    for i in range(len(target.data.tolist())):\n",
        "        true_word = inverse_vocab[target.data.tolist()[i]]\n",
        "        pred_word = inverse_vocab[pred.cpu().data.tolist()[i]]\n",
        "        true_sentence.append(true_word)\n",
        "        pred_sentence.append(pred_word)\n",
        "\n",
        "    print('true:', ' '.join(true_sentence))\n",
        "    print('pred:', ' '.join(pred_sentence), '\\n')"
      ],
      "metadata": {
        "id": "qRq0awC_5P4h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}