{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M162Z3okEGfE"
      },
      "source": [
        "# ニューラルネットワークによるMNISTデータセットの認識実験\n",
        "\n",
        "---\n",
        "## 目的\n",
        "多層パーセプトロン (Multi Layer Perceptoron; MLP) を用いてMNISTデータセットに対する文字認識を行う．\n",
        "\n",
        "## 対応するチャプター\n",
        "* 6序文: 全結合型ニューラルネットワーク\n",
        "* 6.2.2: マルチヌーイ分布出力のためのソフトマックスユニット\n",
        "* 6.3 (6.3.1): ReLUとその一般化\n",
        "* 6.3 (6.3.2): ロジスティックシグモイドとハイパボリックタンジェント（tanh）\n",
        "* 10.9 (10.9.2): 複数時間スケールのためのLeakyユニットとその他の手法（Leaky ReLU）\n",
        "* 8.1.3: バッチアルゴリズムとミニバッチアルゴリズム\n",
        "* 8.3.1: 確率的勾配降下法\n",
        "\n",
        "## モジュールのインポート\n",
        "プログラムの実行に必要なモジュールをインポートします．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBY7X_KXEGfH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gzip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EM55pxUEGfH"
      },
      "source": [
        "## データセットのダウンロードと読み込み\n",
        "\n",
        "今回の実験では，MNISTデータセットを使用します．\n",
        "\n",
        "[MNIST dataset](http://yann.lecun.com/exdb/mnist/)\n",
        "\n",
        "まずはじめに，`wget`コマンドを使用して，MNISTデータセットをダウンロードします．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJdUvfCiEGfI"
      },
      "outputs": [],
      "source": [
        "!wget https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz -O train-images-idx3-ubyte.gz\n",
        "!wget https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz -O train-labels-idx1-ubyte.gz\n",
        "!wget https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz -O t10k-images-idx3-ubyte.gz\n",
        "!wget https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz -O t10k-labels-idx1-ubyte.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Umxv5SQjEGfI"
      },
      "source": [
        "次に，ダウンロードしたファイルからデータを読み込みます．\n",
        "読み込み方は，公式のHPに公開されているように，`gzip`でファイルを展開し，`frombuffer`関数でnumpy arrayとして読み込みます．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MymJxvyvEGfI"
      },
      "outputs": [],
      "source": [
        "# load images\n",
        "with gzip.open('train-images-idx3-ubyte.gz', 'rb') as f:\n",
        "    x_train = np.frombuffer(f.read(), np.uint8, offset=16)\n",
        "x_train = x_train.reshape(-1, 784)\n",
        "\n",
        "with gzip.open('t10k-images-idx3-ubyte.gz', 'rb') as f:\n",
        "    x_test = np.frombuffer(f.read(), np.uint8, offset=16)\n",
        "x_test = x_test.reshape(-1, 784)\n",
        "\n",
        "with gzip.open('train-labels-idx1-ubyte.gz', 'rb') as f:\n",
        "    y_train = np.frombuffer(f.read(), np.uint8, offset=8)\n",
        "\n",
        "with gzip.open('t10k-labels-idx1-ubyte.gz', 'rb') as f:\n",
        "    y_test = np.frombuffer(f.read(), np.uint8, offset=8)\n",
        "\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-__HgWFEGfJ"
      },
      "source": [
        "### MNISTデータセットの表示\n",
        "MNISTデータセットに含まれる画像を表示してみます．\n",
        "ここでは，matplotlibを用いて複数の画像を表示させるプログラムを利用します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQfs0BKpEGfJ"
      },
      "outputs": [],
      "source": [
        "plt.clf()\n",
        "fig = plt.figure(figsize=(14, 1.4))\n",
        "for c in range(10):\n",
        "    ax = fig.add_subplot(1, 10, c + 1)\n",
        "    ax.imshow(x_train[c].reshape(28, 28), cmap=plt.get_cmap('gray'))\n",
        "    ax.set_axis_off()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nADMryKvEGfK"
      },
      "source": [
        "## ネットワークモデルの定義\n",
        "次に，ニューラルネットワーク（多層パーセプトロン）を定義します．\n",
        "\n",
        "まずはじめに，ネットワークの定義に必要な関数を定義します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cfwDoWWEGfK"
      },
      "outputs": [],
      "source": [
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_grad(x):\n",
        "    grad = np.zeros(x.shape)\n",
        "    grad[x >= 0] = 1\n",
        "    return grad\n",
        "\n",
        "def leaky_relu(x, negative_slope=0.01):\n",
        "    return np.maximum(0, x) + 0.01 * np.minimum(0, x)\n",
        "    \n",
        "def leaky_relu_grad(x, negative_slope=0.01):\n",
        "    grad = np.zeros(x.shape)\n",
        "    grad[x >= 0] = 1\n",
        "    grad[x < 0] = negative_slope\n",
        "    return grad\n",
        "\n",
        "def tanh(x):\n",
        "    return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
        "    \n",
        "def tanh_grad(x):\n",
        "    return 1 - x**2\n",
        "\n",
        "def softmax(x):\n",
        "    if x.ndim == 2:\n",
        "        x = x.T\n",
        "        x = x - np.max(x, axis=0)\n",
        "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "        return y.T \n",
        "\n",
        "    x = x - np.max(x)\n",
        "    return np.exp(x) / np.sum(np.exp(x))\n",
        "\n",
        "def cross_entropy(y, t):\n",
        "    if y.ndim == 1:\n",
        "        t = t.reshape(1, t.size)\n",
        "        y = y.reshape(1, y.size)\n",
        "\n",
        "    # 教師データがone-hot-vectorの場合、正解ラベルのインデックスに変換\n",
        "    if t.size == y.size:\n",
        "        t = t.argmax(axis=1)\n",
        "\n",
        "    batch_size = y.shape[0]\n",
        "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
        "\n",
        "def softmax_cross_entropy(x, t):\n",
        "    y = softmax(x)\n",
        "    return cross_entropy(y, t)\n",
        "\n",
        "def multiclass_classification_accuracy(pred, true):\n",
        "    clf_res = np.argmax(pred, axis=1)\n",
        "    return np.sum(clf_res == true).astype(np.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "us3CvpjxEGfK"
      },
      "source": [
        "次に，上で定義した関数を用いてネットワークを定義します．\n",
        "ここでは，入力層，中間層，出力層から構成される多層パーセプトロンとします．\n",
        "\n",
        "入力層と中間層，出力層のユニット数は引数として与え，それぞれ`input_size`，`hidden_size`, `output_size`とします．\n",
        "また，ネットワーク内で使用する活性化関数を選択する`act_func`を`['relu', 'lrelu', 'tanh']`のうちから一つ選択します．\n",
        "活性化関数に`lrelu`（Leaky ReLU）を選択した場合の負の値に対する漏れ値（`negative_slope`）を設定します．\n",
        "そして，`__init__`関数を用いて，ネットワークのパラメータを初期化します．\n",
        "`w1`および`w2`は各層の重みで，`b1`および`b2`はバイアスを表しています．\n",
        "重みは`randn`関数で，標準正規分布に従った乱数で生成した値を保有する配列を生成します．\n",
        "バイアスは`zeros`関数を用いて，要素が全て0の配列を生成します．\n",
        "\n",
        "そして，`forward`関数で，データを入力して結果を出力するための演算を定義します．\n",
        "\n",
        "次に，`backward`関数ではパラメータの更新量を計算します．\n",
        "まず，ネットワークの出力結果と教師ラベルから，誤差`dy`を算出します．\n",
        "この時，教師ラベルをone-hotベクトルへ変換し，各ユニットの出力との差を取ることで，`dy`を計算しています．\n",
        "その後，連鎖律に基づいて，出力層から順番に勾配を計算していきます．\n",
        "このとき，パラメータの更新量を`self.grads`へ保存しておきます．\n",
        "\n",
        "最後に`update_parameters`関数で，更新量をもとにパラメータの更新を行います．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Th_YlJ4kEGfL"
      },
      "outputs": [],
      "source": [
        "class MLPMultinoulli:\n",
        "    \n",
        "    def __init__(self, input_size, hidden_size, output_size, act_func='relu', negative_slope=0.01, w_std=0.01):\n",
        "        \n",
        "        assert act_func in ['relu', 'lrelu', 'tanh'], \"act_func should be selected from ['relu', 'lrelu', 'tanh']\"\n",
        "        self.act_func = act_func\n",
        "        self.negative_slope = negative_slope\n",
        "        \n",
        "        self.w1 = w_std * np.random.randn(input_size, hidden_size)\n",
        "        self.b1 = np.zeros(hidden_size)\n",
        "        self.w2 = w_std * np.random.randn(hidden_size, output_size)\n",
        "        self.b2 = np.zeros(output_size)\n",
        "        \n",
        "        if self.act_func == 'relu':\n",
        "            self.act = relu\n",
        "            self.act_grad = relu_grad\n",
        "        elif self.act_func == 'lrelu':\n",
        "            self.act = leaky_relu\n",
        "            self.act_grad = leaky_relu_grad\n",
        "        elif self.act_func == 'tanh':\n",
        "            self.act = tanh\n",
        "            self.act_grad = tanh_grad\n",
        "\n",
        "        self.grads = {}\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.h1 = np.dot(x, self.w1) + self.b1\n",
        "        if self.act_func == 'lrelu':\n",
        "            self.h2 = self.act(self.h1, self.negative_slope)\n",
        "        else:\n",
        "            self.h2 = self.act(self.h1)\n",
        "        self.h3 = np.dot(self.h2, self.w2) + self.b2\n",
        "        return self.h3\n",
        "\n",
        "    def backward(self, x, t):\n",
        "        batch_size = x.shape[0]\n",
        "        \n",
        "        # forward\n",
        "        _ = self.forward(x)\n",
        "        y = softmax(self.h3)\n",
        "        \n",
        "        # backward\n",
        "        self.grads = {}\n",
        "        \n",
        "        t = np.identity(10)[t]\n",
        "        \n",
        "        dy = (y - t) / batch_size\n",
        "        \n",
        "        d_h2 = np.dot(dy, self.w2.T)\n",
        "        self.grads['w2'] = np.dot(self.h2.T, dy)\n",
        "        self.grads['b2'] = np.sum(dy, axis=0)\n",
        "        \n",
        "        if self.act_func == 'lrelu':\n",
        "            d_h1 = self.act_grad(self.h2, self.negative_slope) * d_h2\n",
        "        else:\n",
        "            d_h1 = self.act_grad(self.h2) * d_h2\n",
        "        \n",
        "        self.grads['w1'] = np.dot(x.T, d_h1)\n",
        "        self.grads['b1'] = np.sum(d_h1, axis=0)\n",
        "        \n",
        "    def update_parameters(self, lr=0.1):\n",
        "        self.w1 -= lr * self.grads['w1']\n",
        "        self.b1 -= lr * self.grads['b1']\n",
        "        self.w2 -= lr * self.grads['w2']\n",
        "        self.b2 -= lr * self.grads['b2']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0j8U6iSfEGfL"
      },
      "source": [
        "## ネットワークの作成と学習の準備\n",
        "上のプログラムで定義したネットワークを作成します．\n",
        "\n",
        "\n",
        "まず，中間層と出力層のユニット数を定義します．\n",
        "ここでは，入力層のユニット数`input_size`を学習データの次元，中間層のユニット数`hidden_size`を128，出力層のユニット数`output_size`を10とします．\n",
        "活性化関数はReLUを使用します．\n",
        "\n",
        "各層のユニット数を`MLPMultinoulli`クラスの引数として与え，ネットワークを作成します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jbi4yMpfEGfL"
      },
      "outputs": [],
      "source": [
        "input_size = x_train.shape[1]\n",
        "hidden_size = 128\n",
        "output_size = 10\n",
        "model = MLPMultinoulli(input_size=input_size, hidden_size=hidden_size, output_size=output_size, act_func='tanh')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k87CokYhEGfM"
      },
      "source": [
        "## 学習\n",
        "読み込んだMNISTデータセットと作成したネットワークを用いて，学習を行います．\n",
        "\n",
        "1回の誤差を算出するデータ数（ミニバッチサイズ）を100，学習エポック数を20とします．\n",
        "\n",
        "学習データは毎回ランダムに決定するため，numpyの`permutation`という関数を利用します．\n",
        "各更新において，学習用データと教師データをそれぞれ`x_batch`と`y_batch`とします．\n",
        "学習モデルに`x_batch`を与えて，`h`を取得します．\n",
        "取得した`h`は精度および誤差を算出するための関数へと入力され，値を保存します．\n",
        "そして，誤差を`backward`関数で逆伝播し，`update_parameters`でネットワークの更新を行います．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVO-JcmZEGfM"
      },
      "outputs": [],
      "source": [
        "num_train_data = x_train.shape[0]\n",
        "batch_size = 100\n",
        "epoch_num = 20\n",
        "\n",
        "for epoch in range(1, epoch_num + 1):\n",
        "    \n",
        "    sum_accuracy = 0.0\n",
        "    sum_loss = 0.0\n",
        "    \n",
        "    perm = np.random.permutation(num_train_data)\n",
        "    for i in range(0, num_train_data, batch_size):\n",
        "        x_batch = x_train[perm[i:i+batch_size]]\n",
        "        y_batch = y_train[perm[i:i+batch_size]]\n",
        "        \n",
        "        h = model.forward(x_batch)\n",
        "        sum_accuracy += multiclass_classification_accuracy(h, y_batch)\n",
        "        sum_loss += softmax_cross_entropy(h, y_batch)\n",
        "        \n",
        "        model.backward(x_batch, y_batch)\n",
        "        model.update_parameters(lr=0.001)\n",
        "        \n",
        "    print(\"epoch: {}, mean loss: {}, mean accuracy: {}\".format(epoch,\n",
        "                                                               sum_loss / num_train_data,\n",
        "                                                               sum_accuracy / num_train_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsQkYyd3EGfM"
      },
      "source": [
        "## テスト\n",
        "学習したネットワークを用いて，テストデータに対する認識率の確認を行います．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ksj1rD_bEGfM"
      },
      "outputs": [],
      "source": [
        "count = 0\n",
        "num_test_data = x_test.shape[0]\n",
        "\n",
        "for i in range(num_test_data):\n",
        "    x = np.array([x_test[i]], dtype=np.float32)\n",
        "    t = y_test[i]\n",
        "    y = model.forward(x)\n",
        "    \n",
        "    pred = np.argmax(y.flatten())\n",
        "    \n",
        "    if pred == t:\n",
        "        count += 1\n",
        "        \n",
        "print(\"test accuracy: {}\".format(count / num_test_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zrAgDgoEGfM"
      },
      "source": [
        "## 課題\n",
        "1. ネットワークの中間層のユニット数を変更して認識性能の変化を確認しましょう\n",
        "2. 使用する活性化関数を変更して認識性能の変化を確認しましょう"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "05_neural_network_multinoulli.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}