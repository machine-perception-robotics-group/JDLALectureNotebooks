{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "910756f7-12ef-4246-96f6-8008a48ca044",
      "metadata": {
        "id": "910756f7-12ef-4246-96f6-8008a48ca044"
      },
      "source": [
        "# Bidirectional Encoder Representations from Transformers (BERT)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 目的\n",
        "Bidirectional Encoder Representations from Transformers (BERT) を理解する．\n",
        "\n",
        "\n",
        "\n",
        "## モジュールのインポート・データのダウンロード\n",
        "演習に使用するモジュールとデータをダウンロードします．\n",
        "\n",
        "## GPUの確認\n",
        "続いてGPUを使用した計算が可能かどうかを確認します．\n",
        "\n",
        "`Use CUDA: True`と表示されれば，GPUを使用した計算をChainerで行うことが可能です．\n",
        "Falseとなっている場合は，上記の「Google Colaboratoryの設定確認・変更」に記載している手順にしたがって，設定を変更した後に，モジュールのインポートから始めてください．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f699d960-e64f-44c2-aeb8-00dbaa483d6c",
      "metadata": {
        "id": "f699d960-e64f-44c2-aeb8-00dbaa483d6c"
      },
      "outputs": [],
      "source": [
        "!pip install transformers fugashi ipadic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26044fb5-695a-4859-88c1-d26a8c4fa59d",
      "metadata": {
        "id": "26044fb5-695a-4859-88c1-d26a8c4fa59d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import re\n",
        "import sys\n",
        "import csv\n",
        "import tarfile\n",
        "\n",
        "# GPUの使用設定の確認\n",
        "use_cuda = torch.cuda.is_available()\n",
        "print('Use CUDA:', use_cuda)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79fa0166-4a8d-44e5-943a-7cbd14019278",
      "metadata": {
        "id": "79fa0166-4a8d-44e5-943a-7cbd14019278"
      },
      "source": [
        "## データセットの作成\n",
        "\n",
        "ダウンロードしたデータを読み込んで，学習データの作成を行います．\n",
        "\n",
        "今回使用するデータは，[livedoorニュースコーパスデータセット](https://www.rondhuit.com/download.html#ldcc)です．\n",
        "これは，Web上のweb記事のデータセットとなっており，複数のカテゴリのニュース記事から構成されています．\n",
        "今回はこのうち，「ITLife Hack（IT関連記事）」と「Sports Watch（スポーツ関連記事）」を分類するタスクおよびデータセットを扱います．\n",
        "\n",
        "まずデータをダウンロードし，実験に使用するデータ飲みを抽出し，テキストファイルに一度書き出します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7790005f-b5c4-4527-bbe1-03de3a38d49a",
      "metadata": {
        "id": "7790005f-b5c4-4527-bbe1-03de3a38d49a"
      },
      "outputs": [],
      "source": [
        "tgz_fname = \"ldcc-20140209.tar.gz\"              # ダウンロードした圧縮ファイルのパスを設定\n",
        "target_genre = [\"it-life-hack\", \"sports-watch\"] # 2つをニュースメディアのジャンルを選定\n",
        "tsv_fname = \"all_text.tsv\"                      # 処理をした結果を保存するファイル名 \n",
        "\n",
        "# データのダウンロード（カレントディレクトリに圧縮ファイルがダウンロードされる）\n",
        "urllib.request.urlretrieve(\"https://www.rondhuit.com/download/ldcc-20140209.tar.gz\", \"ldcc-20140209.tar.gz\")\n",
        "\n",
        "# 処理部分 -------\n",
        "brackets_tail = re.compile('【[^】]*】$')\n",
        "brackets_head = re.compile('^【[^】]*】')\n",
        "\n",
        "def remove_brackets(inp):\n",
        "    output = re.sub(brackets_head, '', re.sub(brackets_tail, '', inp))\n",
        "    return output\n",
        "\n",
        "def read_title(f):\n",
        "    # 2行スキップ\n",
        "    next(f)\n",
        "    next(f)\n",
        "    title = next(f) # 3行目を返す\n",
        "    title = remove_brackets(title.decode('utf-8'))\n",
        "    return title[:-1]\n",
        "\n",
        "zero_fnames = []\n",
        "one_fnames = []\n",
        "\n",
        "with tarfile.open(tgz_fname) as tf:\n",
        "    # 対象ファイルの選定\n",
        "    for ti in tf:\n",
        "        # ライセンスファイルはスキップ\n",
        "        if \"LICENSE.txt\" in ti.name:\n",
        "            continue\n",
        "        if target_genre[0] in ti.name and ti.name.endswith(\".txt\"):\n",
        "            zero_fnames.append(ti.name)\n",
        "            continue\n",
        "        if target_genre[1] in ti.name and ti.name.endswith(\".txt\"):\n",
        "            one_fnames.append(ti.name)\n",
        "    with open(tsv_fname, \"w\") as wf:\n",
        "        writer = csv.writer(wf, delimiter='\\t')\n",
        "        # ラベル 0\n",
        "        for name in zero_fnames:\n",
        "            f = tf.extractfile(name)\n",
        "            title = read_title(f)\n",
        "            row = [target_genre[0], 0, '', title]\n",
        "            writer.writerow(row)\n",
        "        # ラベル 1\n",
        "        for name in one_fnames:\n",
        "            f = tf.extractfile(name)\n",
        "            title = read_title(f)\n",
        "            row = [target_genre[1], 1, '', title]\n",
        "            writer.writerow(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3ccf266-4ec7-4fd6-a074-834a80eec3d9",
      "metadata": {
        "id": "d3ccf266-4ec7-4fd6-a074-834a80eec3d9"
      },
      "source": [
        "### データの表示と確認\n",
        "\n",
        "抽出したデータを読み込み，一部を表示して内容を確認します．\n",
        "\n",
        "表示すると，文章と記事のクラスなどがまとめられていることが確認できます．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a24d6671-3fb9-4c6b-9473-8f3833a08fd9",
      "metadata": {
        "id": "a24d6671-3fb9-4c6b-9473-8f3833a08fd9"
      },
      "outputs": [],
      "source": [
        "# データの読み込み\n",
        "df = pd.read_csv(\"all_text.tsv\", \n",
        "                 delimiter='\\t', header=None, names=['media_name', 'label', 'NaN', 'sentence'])\n",
        "\n",
        "# データの確認\n",
        "print(f'データサイズ： {df.shape}')\n",
        "df.sample(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d39ade6a-ca47-4b57-bde0-c3a067aca467",
      "metadata": {
        "id": "d39ade6a-ca47-4b57-bde0-c3a067aca467"
      },
      "source": [
        "### データの抽出\n",
        "\n",
        "上記では，pandasのDataFrameでデータを読み込んでいるため，\n",
        "必要なデータのみをDataFrameから抽出します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "793aba4c-2aa0-4d24-ad3c-29333ef0d06d",
      "metadata": {
        "id": "793aba4c-2aa0-4d24-ad3c-29333ef0d06d"
      },
      "outputs": [],
      "source": [
        "# データの抽出\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6da65742-9095-4a8d-974e-e34a643451dd",
      "metadata": {
        "id": "6da65742-9095-4a8d-974e-e34a643451dd"
      },
      "source": [
        "### 日本語文章の分解とIDへの変換\n",
        "\n",
        "次に，データセットの日本語文章を単語に分解し，各単語に対応するIDへ変換を行います\n",
        "\n",
        "この処理には，`BertJapaneseTokenizer`を用いて変換します．\n",
        "\n",
        "まず，`BertJapaneseTokenizer`のインスタンスを作成します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d201110-e084-4e3c-937b-add9bba946f6",
      "metadata": {
        "id": "7d201110-e084-4e3c-937b-add9bba946f6"
      },
      "outputs": [],
      "source": [
        "from transformers import BertJapaneseTokenizer\n",
        "tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8debb022-8e6b-45d0-aaa8-b7286f616b1c",
      "metadata": {
        "id": "8debb022-8e6b-45d0-aaa8-b7286f616b1c"
      },
      "source": [
        "次に作成したTokenizerを使用して，日本語文章を単語およびIDへの変換結果を確認してみます．\n",
        "\n",
        "単語に分解されていることが確認できます．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72e74348-5e70-4905-b7d5-fba174822695",
      "metadata": {
        "id": "72e74348-5e70-4905-b7d5-fba174822695"
      },
      "outputs": [],
      "source": [
        "# 元文章\n",
        "print('Original: ', sentences[0])\n",
        "# Tokenizer\n",
        "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
        "# Token-id\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89154a7a-ef1d-43b8-b2df-164cc47e4227",
      "metadata": {
        "id": "89154a7a-ef1d-43b8-b2df-164cc47e4227"
      },
      "source": [
        "次にデータセット全ての文章を単語へ分解しID変換したデータを作成します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57f54248-3c8a-4364-bf8c-0773c2a9d29b",
      "metadata": {
        "id": "57f54248-3c8a-4364-bf8c-0773c2a9d29b"
      },
      "outputs": [],
      "source": [
        "# 最大単語数の確認\n",
        "max_len = []\n",
        "\n",
        "# 1文づつ処理\n",
        "for sent in sentences:\n",
        "    # Tokenizeで分割\n",
        "    token_words = tokenizer.tokenize(sent)\n",
        "    # 文章数を取得してリストへ格納\n",
        "    max_len.append(len(token_words))\n",
        "\n",
        "# 最大の値を確認\n",
        "print('最大単語数:', max(max_len))\n",
        "print('注意：上記の最大単語数にSpecial token（[CLS], [SEP]）の+2をした値が最大単語数')\n",
        "\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# 1文づつ処理\n",
        "for sent in sentences:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      \n",
        "                        add_special_tokens = True,      # Special Tokenの追加\n",
        "                        max_length = 37,                # 文章の長さを固定（Padding/Trancatinating）\n",
        "                        pad_to_max_length = True,       # PADDINGで埋める\n",
        "                        return_attention_mask = True,   # Attention maksの作成\n",
        "                        return_tensors = 'pt',          #  Pytorch tensorsで返す\n",
        "                   )\n",
        "\n",
        "    # 単語IDを取得    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "\n",
        "    # Attention maskの取得\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# リストに入ったtensorを縦方向（dim=0）へ結合\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "\n",
        "# tenosor型に変換\n",
        "labels = torch.tensor(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "449985e2-9bb8-45d5-8c7e-1c45f004100e",
      "metadata": {
        "id": "449985e2-9bb8-45d5-8c7e-1c45f004100e"
      },
      "source": [
        "### データセットクラスの作成\n",
        "\n",
        "読み込み・整理をしたデータを用いて，データセットクラスを作成します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c41c13dc-86b0-459c-a3fc-4e8c5ed4f8da",
      "metadata": {
        "id": "c41c13dc-86b0-459c-a3fc-4e8c5ed4f8da"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# データセットクラスの作成\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "# 90%地点のIDを取得\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "# データセットを分割\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('訓練データ数: {}'.format(train_size))\n",
        "print('検証データ数: {}'.format(val_size))\n",
        "\n",
        "# データローダーの作成\n",
        "batch_size = 32\n",
        "\n",
        "# 訓練データローダー\n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  \n",
        "            sampler = RandomSampler(train_dataset),   # ランダムにデータを取得してバッチ化\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "\n",
        "# 検証データローダー\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, \n",
        "            sampler = SequentialSampler(val_dataset), # 順番にデータを取得してバッチ化\n",
        "            batch_size = batch_size\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01a68100-ff89-4f2f-a4c6-58882b935306",
      "metadata": {
        "id": "01a68100-ff89-4f2f-a4c6-58882b935306"
      },
      "source": [
        "## ネットワークモデルの作成\n",
        "\n",
        "ネットワークモデルを作成します．\n",
        "\n",
        "今回は，BERTの日本語用のPre-trainedモデルを活用し，Fine-Tuningを行うことで，記事の分類を行います．\n",
        "\n",
        "Pre-trainedモデルの読み込みには`BertForSequenceClassification`を活用します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "098d4525-5958-440a-a176-8fe57a43cfcc",
      "metadata": {
        "id": "098d4525-5958-440a-a176-8fe57a43cfcc"
      },
      "outputs": [],
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# BertForSequenceClassification 学習済みモデルのロード\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"cl-tohoku/bert-base-japanese-whole-word-masking\", # 日本語Pretrainedモデルの指定\n",
        "    num_labels = 2,                                    # ラベル数（今回はBinayなので2、数値を増やせばマルチラベルも対応可）\n",
        "    output_attentions = False,                         # アテンションベクトルを出力するか\n",
        "    output_hidden_states = False,                      # 隠れ層を出力するか\n",
        ")\n",
        "\n",
        "# モデルをGPUへ転送\n",
        "model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eae4a8ba-0de3-4654-8751-a87e1e5efc45",
      "metadata": {
        "id": "eae4a8ba-0de3-4654-8751-a87e1e5efc45"
      },
      "source": [
        "## 学習\n",
        "ネットワークを学習（Fine-Tuning）します．\n",
        "\n",
        "最適化手法には，Adamに用いるweight decayを改良した`AdamW`を使用します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "806a2eaa-3ae0-405c-8ab4-1503f2ece0cd",
      "metadata": {
        "id": "806a2eaa-3ae0-405c-8ab4-1503f2ece0cd"
      },
      "outputs": [],
      "source": [
        "# 最適化手法の設定\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "# 訓練パートの定義\n",
        "def train(model):\n",
        "    model.train() # 訓練モードで実行\n",
        "    train_loss = 0\n",
        "    for batch in train_dataloader:# train_dataloaderはword_id, mask, labelを出力する点に注意\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels).loss\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "    return train_loss\n",
        "\n",
        "# テストパートの定義\n",
        "def validation(model):\n",
        "    model.eval()# 訓練モードをオフ\n",
        "    val_loss = 0\n",
        "    with torch.no_grad(): # 勾配を計算しない\n",
        "        for batch in validation_dataloader:\n",
        "            b_input_ids = batch[0].to(device)\n",
        "            b_input_mask = batch[1].to(device)\n",
        "            b_labels = batch[2].to(device)\n",
        "            with torch.no_grad():\n",
        "                loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask,labels=b_labels).loss\n",
        "            val_loss += loss.item()\n",
        "    return val_loss\n",
        "\n",
        "# 学習の実行\n",
        "max_epoch = 4\n",
        "train_loss_ = []\n",
        "test_loss_ = []\n",
        "\n",
        "for epoch in range(max_epoch):\n",
        "    train_ = train(model)\n",
        "    test_ = train(model)\n",
        "    train_loss_.append(train_)\n",
        "    test_loss_.append(test_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27494c56-c45e-447c-86d8-3cdc85f30063",
      "metadata": {
        "id": "27494c56-c45e-447c-86d8-3cdc85f30063"
      },
      "source": [
        "## 検証\n",
        "\n",
        "学習したモデルを検証データで評価します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a1e67fd-effa-4950-aa90-960767fadf93",
      "metadata": {
        "id": "9a1e67fd-effa-4950-aa90-960767fadf93"
      },
      "outputs": [],
      "source": [
        "# 検証方法の確認（1バッチ分で計算ロジックに確認）\n",
        "count = 0\n",
        "model.eval()# 訓練モードをオフ\n",
        "for batch in validation_dataloader:\n",
        "    b_input_ids = batch[0].to(device)\n",
        "    b_input_mask = batch[1].to(device)\n",
        "    b_labels = batch[2].to(device)\n",
        "    with torch.no_grad():   \n",
        "        # 学習済みモデルによる予測結果をpredsで取得     \n",
        "        preds = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "        if count == 0:\n",
        "            PRED = preds[0]\n",
        "            LABEL = b_labels\n",
        "        else:\n",
        "            PRED = torch.cat((PRED, preds[0]), dim=0)\n",
        "            LABEL = torch.cat((LABEL, b_labels), dim=0)\n",
        "        count += 1\n",
        "        \n",
        "\n",
        "# 比較しやすい様にpd.dataframeへ整形\n",
        "# pd.dataframeへ変換（GPUに乗っているTensorはgpu->cpu->numpy->dataframeと変換）\n",
        "logits_df = pd.DataFrame(PRED.cpu().numpy(), columns=['logit_0', 'logit_1'])\n",
        "\n",
        "# np.argmaxで大き方の値を取得\n",
        "pred_df = pd.DataFrame(np.argmax(PRED.cpu().numpy(), axis=1), columns=['pred_label'])\n",
        "label_df = pd.DataFrame(LABEL.cpu().numpy(), columns=['true_label'])\n",
        "\n",
        "print(\"accuracy: \", np.sum(np.argmax(PRED.cpu().numpy(), axis=1) == LABEL.cpu().numpy()) / len(PRED))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2dc59cb-c5f8-428b-9d5a-7557e3d98432",
      "metadata": {
        "id": "f2dc59cb-c5f8-428b-9d5a-7557e3d98432"
      },
      "source": [
        "最後に結果をDataFrameで一覧表示し，確認します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5505b681-d83c-4853-a015-df89de118b72",
      "metadata": {
        "id": "5505b681-d83c-4853-a015-df89de118b72"
      },
      "outputs": [],
      "source": [
        "accuracy_df = pd.concat([logits_df, pred_df, label_df], axis=1)\n",
        "accuracy_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82c8ed40-976a-40de-b43b-13ad912ff7d9",
      "metadata": {
        "id": "82c8ed40-976a-40de-b43b-13ad912ff7d9"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "32_bert.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}