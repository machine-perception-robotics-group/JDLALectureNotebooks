{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/machine-perception-robotics-group/JDLALectureNotebooks/blob/master/notebooks/16_bi-directional_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pq2ahvHHvqeY"
      },
      "source": [
        "# 双方向LSTMによる品詞のタグ付け\n",
        "\n",
        "---\n",
        "## 目的\n",
        "双方向LSTM (Bi-direcitional LSTM) を用いて英単語に対する品詞をタグ付け (POS tagging) を行う．\n",
        "\n",
        "\n",
        "## 対応するチャプター\n",
        "* 10.3: 双方向RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGMW4GMtvqeb"
      },
      "source": [
        "## モジュールのインポート\n",
        "プログラムの実行に必要なモジュールをインポートします．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yg4qgVgtvqeb"
      },
      "outputs": [],
      "source": [
        "from os.path import join\n",
        "from time import time\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuqq_jadvqeb"
      },
      "source": [
        "## GPUの確認\n",
        "GPUを使用した計算が可能かどうかを確認します．\n",
        "\n",
        "`Use CUDA: True`と表示されれば，GPUを使用した計算をChainerで行うことが可能です．\n",
        "Falseとなっている場合は，上記の「Google Colaboratoryの設定確認・変更」に記載している手順にしたがって，設定を変更した後に，モジュールのインポートから始めてください．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNOZi9_Ivqec"
      },
      "outputs": [],
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "print('Use CUDA:', use_cuda)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LczoBn8gyQYM"
      },
      "source": [
        "## データのダウンロードとデータローダーの準備\n",
        "\n",
        "\n",
        "### データのダウンロード\n",
        "実習に必要なデータをダウンロードします．\n",
        "下記のコードを実行してデータのダウンロードを行ってください．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AWJ8V11vqea"
      },
      "outputs": [],
      "source": [
        "!wget http://www.mprg.cs.chubu.ac.jp/~hirakawa/share/tutorial_data/pos-tagging_data.zip\n",
        "!unzip -q -o pos-tagging_data.zip\n",
        "!ls ./pos-tagging_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-oi6QfRvqec"
      },
      "source": [
        "### データローダーの作成\n",
        "\n",
        "データを読み込むためのデータローダーを定義します．\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvmGe73RyeKk"
      },
      "outputs": [],
      "source": [
        "class TextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root=\"./pos-tagging_data\", train=True):\n",
        "        super().__init__()\n",
        "\n",
        "        if train:\n",
        "            self.sentence = self.download_text_dataset(join(root, \"train_sentence.txt\"))\n",
        "            self.label = self.download_text_dataset(join(root, \"train_tags.txt\"))\n",
        "        else:\n",
        "            self.sentence = self.download_text_dataset(join(root, \"test_sentence.txt\"))\n",
        "            self.label = self.download_text_dataset(join(root, \"test_tags.txt\"))\n",
        "\n",
        "        with open(join(root, \"vocab.json\")) as f:\n",
        "            self.vocab = json.load(f)\n",
        "            self.vocab = {v:k for k, v in self.vocab.items()}\n",
        "        _n_vocab = len(self.vocab)\n",
        "        self.vocab[_n_vocab] = \"<PAD>\"\n",
        "        self.vocab_pad_id = _n_vocab\n",
        "        self.n_vocab = len(self.vocab)\n",
        "            \n",
        "        with open(join(root, \"tags.json\")) as f:\n",
        "            self.tags = json.load(f)\n",
        "            self.tags = {v:k for k, v in self.tags.items()}\n",
        "        _n_tags = len(self.tags)\n",
        "        self.tags[_n_tags] = \"<PAD>\"\n",
        "        self.tags_pad_id = _n_tags\n",
        "        self.n_tags = len(self.tags)\n",
        "\n",
        "        # 文章の長さの最大値を検索\n",
        "        self.max_length = 0\n",
        "        for s in self.sentence:\n",
        "            if self.max_length < s.shape[0]:\n",
        "                self.max_length = s.shape[0]\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        _s = self.sentence[item]\n",
        "        _l = self.label[item]\n",
        "        _s = np.pad(_s, (0, self.max_length - _s.shape[0]), constant_values=self.vocab_pad_id)\n",
        "        _l = np.pad(_l, (0, self.max_length - _l.shape[0]), constant_values=self.tags_pad_id)\n",
        "        return _s, _l.astype(np.int64)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentence)\n",
        "\n",
        "    def download_text_dataset(self, input_filename):\n",
        "        downloaded = []\n",
        "        with open(input_filename) as f:\n",
        "            for s in f.readlines():\n",
        "                downloaded.append( np.array(list(map(int, s.strip().split(' '))), dtype=np.int32) )\n",
        "        return downloaded"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "定義したデータローダーから，学習・評価用データセットを準備します．\n",
        "また，呼び出したデータセットの情報を表示して，確認します．"
      ],
      "metadata": {
        "id": "5YLF2SefwX3B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPPa8A4B2SI9"
      },
      "outputs": [],
      "source": [
        "train_data = TextDataset(root=\"./pos-tagging_data\", train=True)\n",
        "test_data = TextDataset(root=\"./pos-tagging_data\", train=False)\n",
        "\n",
        "print(\"Train data -----------\")\n",
        "print(\"    Max length of sentence:\", train_data.max_length)\n",
        "print(\"    The number of vocabularies:\", train_data.n_vocab)\n",
        "print(\"    The number of tags:\", train_data.n_tags)\n",
        "print(\"    Padding ID for input sentence:\", train_data.vocab_pad_id)\n",
        "print(\"    Padding ID for output tag:\", train_data.tags_pad_id)\n",
        "\n",
        "print(\"Test data ------------\")\n",
        "print(\"    Max length of sentence:\", test_data.max_length)\n",
        "print(\"    The number of vocabularies:\", test_data.n_vocab)\n",
        "print(\"    The number of tags:\", test_data.n_tags)\n",
        "print(\"    Padding ID for input sentence:\", test_data.vocab_pad_id)\n",
        "print(\"    Padding ID for output tag:\", test_data.tags_pad_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tclJKQq-vqee"
      },
      "source": [
        "## ネットワークモデルの定義\n",
        "Bidirectional LSTMを用いて，品詞タグ付けを行うためのネットワークを定義します．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLBv3MQDwwnN"
      },
      "outputs": [],
      "source": [
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, n_vocab, n_tags, n_layers, n_units, padding_id):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(n_vocab, n_units, padding_idx=padding_id)\n",
        "        self.bi_lstm = nn.LSTM(n_units, n_units, num_layers=n_layers, batch_first=True, bidirectional=True)\n",
        "        self.output = nn.Linear(2 * n_units, n_tags)\n",
        "\n",
        "    def forward(self, xs):\n",
        "        h = self.embed(xs)\n",
        "        h, _ = self.bi_lstm(h)\n",
        "        h = self.output(h)\n",
        "        return h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvwWYVLmvqee"
      },
      "source": [
        "## ネットワークの作成\n",
        "上のプログラムで定義したネットワークを作成します．\n",
        "\n",
        "学習を行う際の最適化方法としてモーメンタムSGD(モーメンタム付き確率的勾配降下法）を利用します．また，学習率を0.01として引数に与えます．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APfRAMc2vqef"
      },
      "outputs": [],
      "source": [
        "num_vocab = train_data.n_vocab\n",
        "num_tags = train_data.n_tags\n",
        "\n",
        "model = BiLSTM(num_vocab, num_tags, 2, 512, padding_id=train_data.vocab_pad_id)\n",
        "if use_cuda:\n",
        "    model = model.cuda()\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUJlgtjhvqef"
      },
      "source": [
        "## 学習\n",
        "\n",
        "学習を実行します．\n",
        "\n",
        "※ 学習には時間を要します．\n",
        "演習時間の都合で十分な学習が行えない場合は，下記にある学習済みモデルを用いたテストで結果を確認してください．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RLJ-jjc2kQN"
      },
      "outputs": [],
      "source": [
        "# ミニバッチサイズ・エポック数．学習データ数の設定\n",
        "batch_size = 64\n",
        "epoch_num = 100\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "if use_cuda:\n",
        "    criterion = criterion.cuda()\n",
        "\n",
        "model.train()\n",
        "\n",
        "\n",
        "start = time()\n",
        "for epoch in range(1, epoch_num + 1):\n",
        "    sum_loss = 0.0\n",
        "\n",
        "    for input, label in train_loader:\n",
        "        \n",
        "        if use_cuda:\n",
        "            input = input.cuda()\n",
        "            label = label.cuda()\n",
        "\n",
        "        y = model(input)\n",
        "\n",
        "        loss = criterion(y.permute(0, 2, 1), label)\n",
        "\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        sum_loss += loss.item()\n",
        "\n",
        "    print(\"epoch: {}, mean loss: {}, elapsed time: {}\".format(epoch,\n",
        "                                                              sum_loss/len(train_loader),\n",
        "                                                              time() - start))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opXZwRxkvqef"
      },
      "source": [
        "## テスト\n",
        "\n",
        "学習後のネットワークを用いて，品詞のタグづけを行います．\n",
        "\n",
        "ここではテストデータのうち10個の文章に対するタグづけの結果を示しています．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R84PcYoiEYgt"
      },
      "outputs": [],
      "source": [
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=1, shuffle=False)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for count, (input, label) in enumerate(test_loader):\n",
        "        if use_cuda:\n",
        "            input = input.cuda()\n",
        "            label = label.cuda()\n",
        "\n",
        "        y = model(input)\n",
        "\n",
        "        pred = torch.argmax(y, dim=2)\n",
        "\n",
        "        if use_cuda:\n",
        "            input = input.data.cpu().numpy()\n",
        "            pred = pred.data.cpu().numpy()\n",
        "            label = label.data.cpu().numpy()\n",
        "\n",
        "        input_sentence = [test_data.vocab[i] for i in input.flatten()]\n",
        "        pred_tags = [test_data.tags[i] for i in pred.flatten()]\n",
        "        true_tags = [test_data.tags[i] for i in label.flatten()]\n",
        "\n",
        "        last_word_index = np.min(np.where(input.flatten() == test_data.vocab_pad_id))\n",
        "\n",
        "        print(\"input sentence:\", \" \".join(input_sentence[:last_word_index]))\n",
        "        print(\"predicted POS :\", \" \".join(pred_tags[:last_word_index]))\n",
        "        print(\"true POS      :\", \" \".join(true_tags[:last_word_index]) + \"\\n\")\n",
        "\n",
        "        if count == 9:\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clhOGYUavqeg"
      },
      "source": [
        "## テスト（学習済みモデル）\n",
        "\n",
        "学習には時間を要するため，下記のコードでは学習済みのモデルを読み込んで，学習後のネットワークでの翻訳結果を確認します．\n",
        "保存したモデルパラメータを読み込んで，テストデータの翻訳を行います．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adyr90jpvqeg"
      },
      "outputs": [],
      "source": [
        "num_vocab = train_data.n_vocab\n",
        "num_tags = train_data.n_tags\n",
        "model_pretrain = BiLSTM(num_vocab, num_tags, 2, 512, padding_id=train_data.vocab_pad_id)\n",
        "model_pretrain.load_state_dict(torch.load(\"pos-tagging_data/bilstm.pth\"))\n",
        "if use_cuda:\n",
        "    model_pretrain.cuda()\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=1, shuffle=False)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for count, (input, label) in enumerate(test_loader):\n",
        "        if use_cuda:\n",
        "            input = input.cuda()\n",
        "            label = label.cuda()\n",
        "\n",
        "        y = model_pretrain(input)\n",
        "\n",
        "        pred = torch.argmax(y, dim=2)\n",
        "\n",
        "        if use_cuda:\n",
        "            input = input.data.cpu().numpy()\n",
        "            pred = pred.data.cpu().numpy()\n",
        "            label = label.data.cpu().numpy()\n",
        "\n",
        "        input_sentence = [test_data.vocab[i] for i in input.flatten()]\n",
        "        pred_tags = [test_data.tags[i] for i in pred.flatten()]\n",
        "        true_tags = [test_data.tags[i] for i in label.flatten()]\n",
        "\n",
        "        last_word_index = np.min(np.where(input.flatten() == test_data.vocab_pad_id))\n",
        "\n",
        "        print(\"input sentence:\", \" \".join(input_sentence[:last_word_index]))\n",
        "        print(\"predicted POS :\", \" \".join(pred_tags[:last_word_index]))\n",
        "        print(\"true POS      :\", \" \".join(true_tags[:last_word_index]) + \"\\n\")\n",
        "\n",
        "        if count == 9:\n",
        "            break"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "16_bi_directional_lstm.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}