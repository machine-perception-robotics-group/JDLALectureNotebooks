{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/machine-perception-robotics-group/JDLALectureNotebooks/blob/master/notebooks/17_lstm_encoder_decoder_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder-Decoderによる機械翻訳\n",
        "\n",
        "---\n",
        "## 目的\n",
        "Encoder-Decoder構造のネットワークである（Sequence-to-Sequenel Seq2Seq）を用いて機械翻訳（English-French）を行う．\n",
        "\n",
        "\n",
        "## 対応するチャプター\n",
        "* 10.4: Encoder-DecoderとSequence-to-Sequence\n",
        "\n",
        "## データのダウンロード\n",
        "実習に必要なデータをダウンロードします．\n",
        "下記のコードを実行してデータのダウンロードを行ってください．"
      ],
      "metadata": {
        "id": "prsIbOdms7_F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVkSSPV4SfLq"
      },
      "outputs": [],
      "source": [
        "!wget http://www.mprg.cs.chubu.ac.jp/~hirakawa/share/tutorial_data/fra-eng-tr_data.zip\n",
        "!unzip -q -o fra-eng-tr_data.zip\n",
        "!ls ./fra-eng-tr_data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## モジュールのインポート\n",
        "プログラムの実行に必要なモジュールをインポートします．"
      ],
      "metadata": {
        "id": "lbPmsL8ftCAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from time import time\n",
        "import json\n",
        "import numpy\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.utils.rnn as rnn"
      ],
      "metadata": {
        "id": "SaVzNueStAIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## データセットの読み込み\n",
        "データセットを読み込みます．\n",
        "今回のデータはフランス語と英語の対訳データセットを使用します．\n",
        "使用するデータには，学習データとして8479，テストデータとして2120の対訳文が含まれています．\n",
        "\n",
        "また，このデータセットに存在する単語の情報を取得します．\n",
        "`fra_vocab`にはフランス語の単語情報，`eng_vocab`には英語の単語情報を含んだ辞書オブジェクトが読み込まれます．"
      ],
      "metadata": {
        "id": "s768sK4xtFYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_text_dataset(input_filename):\n",
        "    downloaded = []\n",
        "    with open(input_filename) as f:\n",
        "        for s in f.readlines():\n",
        "            downloaded.append( np.array(list(map(int, s.strip().split(' '))), dtype=np.int32) )\n",
        "    return downloaded\n",
        "\n",
        "fra_train = download_text_dataset(\"fra-eng-tr_data/fra_train.txt\")\n",
        "eng_train = download_text_dataset(\"fra-eng-tr_data/eng_train.txt\")\n",
        "fra_test = download_text_dataset(\"fra-eng-tr_data/fra_test.txt\")\n",
        "eng_test = download_text_dataset(\"fra-eng-tr_data/eng_test.txt\")\n",
        "\n",
        "print(len(fra_train))\n",
        "print(len(fra_test))\n",
        "        \n",
        "with open(\"fra-eng-tr_data/fra_vocab.json\") as f:\n",
        "    fra_vocab = json.load(f)\n",
        "    fra_vocab = {int(k):v for k, v in fra_vocab.items()}\n",
        "    \n",
        "with open(\"fra-eng-tr_data/eng_vocab.json\") as f:\n",
        "    eng_vocab = json.load(f)\n",
        "    eng_vocab = {int(k):v for k, v in eng_vocab.items()}"
      ],
      "metadata": {
        "id": "151ZFO_LtD1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ネットワークモデルの定義\n",
        "Seq2Seqのネットワークを定義します．\n",
        "\n",
        "文章の終わりを表現する単語IDとして`EOS=1`を定義します．\n",
        "\n",
        "次に，`sequence_embed`関数を定義します．\n",
        "この関数では，この次に定義するネットワーク内部での計算を行う際に，任意の長さの文章をまとめたミニバッチデータに対して，ある処理を行うための関数です．\n",
        "この関数を用いてネットワークの演算を行うことで，ミニバッチ中に含まれる文章の長さが異なる場合でも，一度に処理を行うことが可能となります．\n",
        "\n",
        "翻訳を行うための`Seq2seq`ネットワークを定義します．\n",
        "このネットワークには，入力された文章（フランス語文）を処理するEncoderと翻訳結果（英文）を出力するDecoderが存在します．\n",
        "\n",
        "まず`__init__`関数でネットワーク内部の層を定義します．\n",
        "`__init__`関数の引数として，次の引数を準備します．\n",
        "`n_layers`はEncoderおよびDecoderに用いるLSTMの総数，`n_source_vocab`は入力側で扱う単語数，`n_target_vocab`は出力側で扱う単語数，`n_units`はLSTMの隠れ層のユニット数です．\n",
        "`embed_x`および`embed_y`ではそれぞれ，入力・出力側で扱う単語のIDから単語を表現した特徴ベクトルを出力するための`Embedding`層を定義します．\n",
        "この`Embedding`層に入る単語IDで埋め込みベクトルを計算したくないもの(例えばパディングしたID)を`padding_idx`で設定できます．\n",
        "次に，`encoder`および`decoder`では，LSTM層を定義します．\n",
        "ここでは，`nn.LSTM`という層を使用して定義を行います．\n",
        "`nn.LSTM`は，文章の長さを揃え，ミニバッチを同時に扱うことができるLSTMの定義方法です．\n",
        "ここでは，LSTMの層数やdropoutのdrop率も同時に定義することが可能です．\n",
        "しかし，`nn.LSTM`はミニバッチ内で文章の長さが統一されてない場合に処理できません．\n",
        "そこで，`pad_sequence`でミニバッチ内で文章の長さを揃えるようにします．\n",
        "`pad_sequence`はミニバッチ内で最も文章の長さに合わせて短い文章の末尾にパディングを追加し次元を揃えてくれる関数になります．\n",
        "このパディングは任意の数に設定できます．今回は0で設定します．\n",
        "最後に，`decoder`から出力された特徴を元に翻訳結果の単語IDを出力するための出力層`W`を定義します．\n",
        "\n",
        "\n",
        "次に，入力されたデータから結果を出力するための演算を定義します．\n",
        "ここでは，下記で実行する学習のプログラムを簡単にするため，2種類の演算の関数を定義します．\n",
        "`forward`関数では，学習を行う際の演算を定義します．ネットワークの出力結果と正解の翻訳情報から計算した誤差を返す関数として定義します．\n",
        "また，`translate`関数では，ネットワークが予測した翻訳結果を出力するように定義します．"
      ],
      "metadata": {
        "id": "H00CeAyFtJNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EOS = 1\n",
        "\n",
        "def sequence_embed(embed, xs):\n",
        "    x_len = [len(x) for x in xs]\n",
        "    x_section = x_len\n",
        "    ex = embed(torch.cat(xs, dim=0))\n",
        "    exs = torch.split(ex, x_section, 0)\n",
        "    return exs\n",
        "\n",
        "class Seq2seq(nn.Module):\n",
        "\n",
        "    def __init__(self, n_layers, n_source_vocab, n_target_vocab, n_units):\n",
        "        super(Seq2seq, self).__init__()\n",
        "        self.embed_x = nn.Embedding(n_source_vocab, n_units, padding_idx=0)\n",
        "        self.embed_y = nn.Embedding(n_target_vocab, n_units, padding_idx=0)\n",
        "        self.encoder = nn.LSTM(n_units, n_units, num_layers=n_layers, batch_first=True, dropout=0.1)\n",
        "        self.decoder = nn.LSTM(n_units, n_units, num_layers=n_layers, batch_first=True, dropout=0.1)\n",
        "        self.W = nn.Linear(n_units, n_target_vocab)\n",
        "\n",
        "        self.n_layers = n_layers\n",
        "        self.n_units = n_units\n",
        "\n",
        "    def forward(self, xs, ys, criterion):\n",
        "        xs = [torch.flipud(x) for x in xs]\n",
        "\n",
        "        eos = torch.from_numpy(np.array([EOS], np.int64)).cuda()\n",
        "        ys_in = [torch.cat([eos, y], dim=0) for y in ys]\n",
        "        ys_out = [torch.cat([y, eos], dim=0) for y in ys]\n",
        "\n",
        "        exs = sequence_embed(self.embed_x, xs)\n",
        "        eys = sequence_embed(self.embed_y, ys_in)\n",
        "\n",
        "        batch = len(xs)\n",
        "        packed_encode_input = rnn.pad_sequence(exs, batch_first=True, padding_value=0)\n",
        "        packed_decode_input = rnn.pad_sequence(eys, batch_first=True, padding_value=0)\n",
        "        packed_target = rnn.pad_sequence(ys_out, batch_first=True, padding_value=0)\n",
        "\n",
        "        e_output, e_hidden = self.encoder(packed_encode_input)\n",
        "        d_output, d_hidden = self.decoder(packed_decode_input, e_hidden)\n",
        "\n",
        "        output = self.W(d_output)\n",
        "        b, s, c = output.shape\n",
        "        output = output.reshape(b*s, c)\n",
        "        packed_target = packed_target.reshape(b*s)\n",
        "\n",
        "        loss = criterion(output, packed_target)\n",
        "        return loss\n",
        "\n",
        "    def translate(self, xs, max_length=100):\n",
        "        batch = len(xs)\n",
        "        with torch.autograd.no_grad():\n",
        "            xs = [torch.flipud(x) for x in xs]\n",
        "            exs = sequence_embed(self.embed_x, xs)\n",
        "            packed_encode_input = rnn.pad_sequence(exs, batch_first=True, padding_value=0)\n",
        "            e_output, hidden = self.encoder(packed_encode_input)\n",
        "            ys = torch.from_numpy(np.array([[EOS]], np.int64)).cuda()\n",
        "            result = []\n",
        "            for i in range(max_length):\n",
        "                eys = self.embed_y(ys) \n",
        "                d_output, hidden = self.decoder(eys, hidden)\n",
        "                d_output = d_output.view(1, d_output.shape[-1])\n",
        "                output = self.W(d_output)\n",
        "                ys = torch.argmax(output, dim=-1, keepdim=True)\n",
        "                output = torch.argmax(output, dim=-1)\n",
        "                result.append(output)\n",
        "\n",
        "        return result"
      ],
      "metadata": {
        "id": "opyQR0tNtHb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ネットワークの作成\n",
        "上のプログラムで定義したネットワークを作成します．\n",
        "ここでは，GPUで学習を行うために，modelをGPUに送るcuda()関数を利用しています．\n",
        "\n",
        "学習を行う際の最適化方法としてモーメンタムSGD(モーメンタム付き確率的勾配降下法）を利用します．また，学習率を0.001として引数に与えます．"
      ],
      "metadata": {
        "id": "Sl3mqKcdtMvq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_source_vocab = len(fra_vocab)\n",
        "num_target_vocab = len(eng_vocab)\n",
        "model = Seq2seq(n_layers=2,\n",
        "                n_source_vocab=num_source_vocab,\n",
        "                n_target_vocab=num_target_vocab,\n",
        "                n_units=1024)\n",
        "model.cuda()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
      ],
      "metadata": {
        "id": "temQN_6DtLOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 学習\n",
        "\n",
        "学習を実行します．\n",
        "`nn.CrossEntropyLoss`内の`ignore_index`は指定したクラスインデックスを計算しないようにする引数です．\n",
        "今回，ミニバッチ内で文章の長さを揃えるためにパディングを行っており，そのまま計算するとノイズになり得るため，余分なクラスインデックスを計算しないようにします．"
      ],
      "metadata": {
        "id": "i31cpMfHtPnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ミニバッチサイズ・エポック数．学習データ数の設定\n",
        "batch_size = 32\n",
        "epoch_num = 100\n",
        "train_data_num = len(fra_train)\n",
        "num_iter_per_epoch = int(train_data_num / batch_size)\n",
        "\n",
        "# 誤差関数の設定\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "criterion.cuda()\n",
        "\n",
        "model.train()\n",
        "# 学習の実行\n",
        "start = time()\n",
        "for epoch in range(1, epoch_num + 1):\n",
        "    \n",
        "    sum_loss = 0\n",
        "    \n",
        "    perm = np.random.permutation(train_data_num)\n",
        "    for i in range(0, train_data_num, batch_size):\n",
        "        optimizer.zero_grad()\n",
        "        fra_batch = [torch.from_numpy(np.array(fra_train[ii], dtype=np.int64)).cuda() for ii in perm[i:i+batch_size] ]\n",
        "        eng_batch = [torch.from_numpy(np.array(eng_train[ii], dtype=np.int64)).cuda() for ii in perm[i:i+batch_size] ]\n",
        "\n",
        "        loss = model(fra_batch, eng_batch, criterion)        \n",
        "        \n",
        "        sum_loss += loss.data\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    elapsed_time = time() - start\n",
        "    print(\"epoch: {}, mean loss: {}, elapsed_time: {}\".format(epoch,\n",
        "                                                              sum_loss/num_iter_per_epoch,\n",
        "                                                              elapsed_time))\n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "        model_name = \"model_v{}.pt\".format(epoch)\n",
        "        torch.save({'model': model.state_dict()}, model_name)"
      ],
      "metadata": {
        "id": "uD5SBYUqtOzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## テスト\n",
        "\n",
        "学習後のネットワークを用いて，翻訳を行います．"
      ],
      "metadata": {
        "id": "ajIVxeyDtSz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"model_v300.pt\")['model'])\n",
        "model.cuda()\n",
        "model.eval()\n",
        "with torch.autograd.no_grad():\n",
        "    test_index = np.arange(20)\n",
        "    \n",
        "    for i_test in test_index:\n",
        "        fra_batch = [ torch.from_numpy(np.array(fra_test[i_test], dtype=np.int64)).cuda() ]\n",
        "        eng_batch = [ torch.from_numpy(np.array(eng_test[i_test], dtype=np.int64)).cuda() ]\n",
        "    \n",
        "        y = model.translate(fra_batch, max_length=10)\n",
        "\n",
        "        fra_true = [fra_vocab[i] for i in fra_batch[0].detach().cpu().numpy()]\n",
        "        eng_pred = [eng_vocab[i.detach().cpu().numpy()[0]] for i in y]\n",
        "\n",
        "        print(\"input french sentence      :\", \" \".join(fra_true))\n",
        "        print(\"translated english sentence:\", \" \".join(eng_pred), \"\\n\")"
      ],
      "metadata": {
        "id": "eJ0cLD43tSA4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}