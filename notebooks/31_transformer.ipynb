{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2f4bbdc-0d00-48bd-b564-82d3ca7f7b4f",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "---\n",
    "## 目的\n",
    "Transformerネットワークを作成して，その構造と演算の中身について理解する．\n",
    "\n",
    "## 対応するチャプター\n",
    "\n",
    "## モジュールのインポート\n",
    "プログラムの実行に必要なモジュールをインポートします．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faaff0a-b7b7-4645-bf16-156e06faa65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5256c5cf-0f7a-4189-8ab7-135e0aaa5123",
   "metadata": {},
   "source": [
    "## 演習タスクとデータセットの作成\n",
    "\n",
    "今回は文字列としての足し算を行う計算機をTransformerで作成します．\n",
    "\n",
    "まずは，足し算のデータを作成するデータセットクラスを作成します．\n",
    "データは0から9までの数字と加算記号，開始，終了のフラグです．また，３桁の数字の足し算を行うため，各桁の値を１つずつランダムに生成して連結しています．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a657b1b9-074f-41ad-bc17-b75e25a4fc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = {str(i): i for i in range(10)}\n",
    "word2id.update({\"<pad>\": 10, \"+\": 11, \"<eos>\": 12})\n",
    "id2word = {v: k for k, v in word2id.items()}\n",
    "\n",
    "class CalcDataset:\n",
    "\n",
    "    def transform(self, string, seq_len=7):\n",
    "        tmp = []\n",
    "        for i, c in enumerate(string):\n",
    "            try:\n",
    "                tmp.append(word2id[c])\n",
    "            except:\n",
    "                tmp += [word2id[\"<pad>\"]] * (seq_len - i)\n",
    "                break\n",
    "        return tmp\n",
    "\n",
    "    def __init__(self, data_num, train=True):\n",
    "        self.data_num = data_num\n",
    "        self.train = train\n",
    "        self.data = []\n",
    "        self.label = []\n",
    "\n",
    "        for _ in range(data_num):\n",
    "            x = int(\"\".join([random.choice(list(\"0123456789\")) for _ in range(random.randint(1, 3))] ))\n",
    "            y = int(\"\".join([random.choice(list(\"0123456789\")) for _ in range(random.randint(1, 3))] ))\n",
    "            left = (\"{:*<7s}\".format(str(x) + \"+\" + str(y))).replace(\"*\", \"<pad>\")\n",
    "            self.data.append(self.transform(left))\n",
    "\n",
    "            z = x + y\n",
    "            right = (\"{:*<6s}\".format(str(z))).replace(\"*\", \"<pad>\")\n",
    "            right = self.transform(right, seq_len=5)\n",
    "            right = [12] + right\n",
    "            right[right.index(10)] = 12\n",
    "            self.label.append(right)\n",
    "        \n",
    "        self.data = np.asarray(self.data)\n",
    "        self.label = np.asarray(self.label)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        d = self.data[item]\n",
    "        l = self.label[item]\n",
    "        return d, l\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b21d58-be9d-4913-b821-6cc6d6cc3569",
   "metadata": {},
   "source": [
    "## Maskの作成\n",
    "\n",
    "データに対するマスクを作成する関数を定義します．\n",
    "\n",
    "デコーダは未来の情報を伝播しないようにMaskをかけます．\n",
    "\n",
    "下図に示すように，Maskは同じ情報から作成されます．黒丸はマスクされた領域を表します．\n",
    "例えば，「好き」という情報を入力した場合に，残りの「な/動物/は」を参照できません．\n",
    "これは推論時未来の情報が与えられないためです．\n",
    "そのため，Queryでは，入力の時刻より先のMemoryの情報に対してMaskをすることで，未来の情報を伝播させないようにします．\n",
    "このマスクはデコーダのMasked Multi-Head Attentionで使われます．\n",
    "\n",
    "\n",
    "<img src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/143078/4dab846e-19ac-3ed4-e16b-943b5921a47e.jpeg\" width=40%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f20f013-10e5-4bfc-88d5-cae1da711498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_mask(batch_size, src, size):\n",
    "    _mask = src == word2id[\"<pad>\"]\n",
    "    mask = copy.deepcopy(_mask)\n",
    "    mask[_mask == 1] = 0.0\n",
    "    mask[_mask == 0] = 1.0\n",
    "    return mask.reshape([mask.shape[0], 1, mask.shape[1]])\n",
    "\n",
    "def decoder_mask(batch_size, size):\n",
    "    _mask = np.triu(np.ones((size, size)), k=1)\n",
    "    mask = copy.deepcopy(_mask)\n",
    "    mask[_mask == 1] = 0.0\n",
    "    mask[_mask == 0] = 1.0\n",
    "\n",
    "    mask = mask.reshape([1, *mask.shape])\n",
    "    mask = np.repeat(mask, batch_size, axis=0)\n",
    "\n",
    "    return mask\n",
    "\n",
    "def create_masks(batch_size, src, trg):\n",
    "    src_mask = encoder_mask(batch_size, src, src.shape[1])\n",
    "\n",
    "    if trg is not None:\n",
    "        size = trg.shape[1]\n",
    "        np_mask = decoder_mask(batch_size, size)\n",
    "        trg_mask = np_mask\n",
    "    else:\n",
    "        trg_mask = None\n",
    "\n",
    "    return src_mask, trg_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6bda4e-c62d-4ff4-8126-169bc814ea15",
   "metadata": {},
   "source": [
    "## Transformerの作成\n",
    "\n",
    "### 概要\n",
    "2017年に発表されたTransformerは，CNNやRNNなどを用いずAttention機構のみを用いたモデルです．\n",
    "翻訳や文章生成などのタスクでRNNとSeq2seqモデルが主流でしたが，これらのモデルは逐次的に単語を処理するため学習時に並列計算できないという問題がありました．\n",
    "また，長文に対してAttentionが使われていましたが，このAttentionはほとんどRNNと一緒に使われていました．\n",
    "一方で，TransformerはAttention機構だけ使うことで，入出力の文章同士の広範囲な依存関係を捉える構造になっています．\n",
    "\n",
    "モデルはSeq2seqと同様にエンコーダ・デコーダモデルです．\n",
    "エンコーダでは，Multi-Head AttentionとFeed Forwardのブロックを$N$回スタックする構造です．\n",
    "デコーダでは，それに加えMasked Multi-Head Attentionのブロックで構成されています．\n",
    "Masked Multi-Head Attentionの学習時，デコーダは自己回帰を使用せず，全ターゲットを同時に入力し，全ターゲットを同時に予測します．\n",
    "この時，予測すべきターゲットの情報が予測前のデコーダにリークしないようにMaskします．\n",
    "評価時は自己回帰でターゲットを生成します．\n",
    "\n",
    "<img src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/143078/badbcfc3-93c3-eb02-ae96-8b6256397c2a.png\" width=35%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23f4626-4718-4301-b60e-ef1872ed97e7",
   "metadata": {},
   "source": [
    "### 活性化関数・全結合層の定義\n",
    "\n",
    "まず，ネットワークの定義に必要となる関数や基本的な全結合層を定義します．\n",
    "これらの定義や詳細については，前回までの演習を参照してください．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b03ac58-d310-4712-87a3-60a1456bec3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax(x, dim=0):\n",
    "    x_max = np.max(x, axis=dim, keepdims=True)\n",
    "    e_x = np.exp(x - x_max)\n",
    "    x_sum = np.sum(e_x, axis=dim, keepdims=True)\n",
    "    f_x = e_x / x_sum \n",
    "    return f_x\n",
    "\n",
    "class Linear:\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.use_bias = bias\n",
    "\n",
    "        self.weight = np.random.randn(out_features, in_features)\n",
    "        if self.use_bias:\n",
    "            self.bias = np.random.randn(out_features)\n",
    "        else:\n",
    "            self.bias = None\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        if self.use_bias:\n",
    "            return np.dot(x, self.weight.T) + self.bias\n",
    "        else:\n",
    "            return np.dot(x, self.weight.T)\n",
    "        \n",
    "class LinearEmbed:\n",
    "    def __init__(self, in_features, out_features):\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        self.weight = np.random.randn(in_features, out_features)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return np.dot(x, self.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922dfc57-9b46-4576-b23d-8ad86210f79b",
   "metadata": {},
   "source": [
    "### Word EmbeddingとPositional Encoding\n",
    "\n",
    "次に，Transformerへデータが入力された際に用いる，EmbeddingとPositional Encodingを定義します．\n",
    "\n",
    "Embeddingは，各文字に対応するid列が入力され，それらに対してユニークな特徴をエンコードする層です．\n",
    "\n",
    "Positonal Encoderは，Embedingした単語の埋め込み表現に対して，その1に対応した値を足し合わせることで位置情報を与える役割を持つ機構です．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574e1a59-9c3a-44e7-a379-0d3ccf831cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.embed = LinearEmbed(self.num_embeddings, self.embedding_dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h = self._onehot(x)\n",
    "        h = self.embed(h)\n",
    "        h[idx == 10, :] = 0 \n",
    "        return h\n",
    "\n",
    "    def _onehot(self, idx):\n",
    "        return np.eye(self.num_embeddings)[idx]\n",
    "\n",
    "\n",
    "class PositionalEncoder:\n",
    "    def __init__(self, embedding_dim, max_seq_len=200):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        pe = np.zeros((max_seq_len, embedding_dim))\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, self.embedding_dim, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i) / self.embedding_dim)))\n",
    "                pe[pos, i+1] = math.cos(pos / (10000 ** ((2 * (i + 1)) / self.embedding_dim)))\n",
    "        self.pe = np.expand_dims(pe, axis=0)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h = x * math.sqrt(self.embedding_dim)\n",
    "        seq_len = h.shape[1]\n",
    "        pe = self.pe[:, :seq_len]\n",
    "        h = h + pe\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc029f8-aca0-4fea-a840-ce2f75388e81",
   "metadata": {},
   "source": [
    "### Multi-Head Attention\n",
    "\n",
    "#### 1. Scaled Dot-product Attention\n",
    "\n",
    "Scaled Dot-Product Attentionは，Self-Attentionとも呼ばれ，Transformerの重要な機構です．\n",
    "\n",
    "数式は以下の通りです．\n",
    "$$ {\\rm Attention}(Q, K, V)={\\rm softmax} ( \\frac{QK^{T}}{\\sqrt{d_k} } ) V $$\n",
    "\n",
    "ここで，$Q, K, V$はそれぞれQuery，Key，Valueです．また$d_k$はQueryの次元数を表します．この平方根$d_k$は，見てわかるように$Q, K$の特徴量をスケールする役割を持ちます．これは層数，すなわちスタックするブロック数(前述のN)が大きくなると，内積が大きくなり，softmax関数の勾配を計算すると非常に小さい値しか返さないためです．\n",
    "\n",
    "図のように，QueryとKeyが行列乗算で計算された後，dの平方根でスケーリングした後，後述するMaskをかけます．この時，Maskには負の無限大がかけられます．これにより，paddingした領域に対しsoftmax後の値を0に近い出力にすることができます．つまり，padding領域のAttention weightを計算しないようにします．最後にValueとの行列乗算をします．\n",
    "\n",
    "<img src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/143078/7dda5594-5bb2-5e2e-566e-3404d722c827.jpeg\" width=30%>\n",
    "\n",
    "\n",
    "#### 2. Multi-Head Attention\n",
    "Multi-Head Attentionは1つのQuery，Key，Valueを持たせるのではなく，小さいQuery，Key，Valueに分割して，分割した特徴表現を計算します．\n",
    "構造はシンプルで，Linear層とScaled Dot-Product Attentionを分割した構造を持ちます．\n",
    "最終的に，分割した出力を1つにまとめてLinear層に渡します．このようにわざわざ分割する理由ですが，モデルが異なる特徴表現の異なる情報についてAttention weightを計算できるためです．\n",
    "\n",
    "\n",
    "<img src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/143078/3c42b52f-aed4-a5af-955b-1156fc27213b.jpeg\" width=30%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bb4cc5-21ed-4d8e-81ef-07394a1a76ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(q, k, v, d_k, mask=None, dec_mask=False):\n",
    "    scores = np.matmul(q, k.transpose(0, 1, 3, 2)) /  math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        if dec_mask:\n",
    "#             print(\"dec mask shape:\", mask.shape)\n",
    "            mask = mask.reshape([mask.shape[0], 1, mask.shape[1], mask.shape[2]])\n",
    "        else:\n",
    "            mask = np.expand_dims(mask, axis=1)\n",
    "        \n",
    "        scores[:, :, :, mask[0,0,0,:]==1] = -1e9\n",
    "\n",
    "    scores = softmax(scores, dim=-1)\n",
    "\n",
    "    output = np.matmul(scores, v)\n",
    "    return output\n",
    "    \n",
    "\n",
    "class MultiHeadAttention:\n",
    "    \n",
    "    def __init__(self, heads, embedding_dim):\n",
    "        self.h = heads\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.d_k = embedding_dim // heads\n",
    "        \n",
    "        self.q_linear = Linear(embedding_dim, embedding_dim)\n",
    "        self.v_linear = Linear(embedding_dim, embedding_dim)\n",
    "        self.k_linear = Linear(embedding_dim, embedding_dim)\n",
    "        \n",
    "        self.out = Linear(embedding_dim, embedding_dim)\n",
    "        \n",
    "    def __call__(self, q, k, v, mask=None, dec_mask=False):\n",
    "        bs = q.shape[0]\n",
    "        \n",
    "        k = self.k_linear(k).reshape([bs, -1, self.h, self.d_k])\n",
    "        q = self.q_linear(q).reshape([bs, -1, self.h, self.d_k])\n",
    "        v = self.v_linear(v).reshape([bs, -1, self.h, self.d_k])\n",
    "        \n",
    "        k = k.transpose(0, 2, 1, 3)\n",
    "        q = q.transpose(0, 2, 1, 3)\n",
    "        v = v.transpose(0, 2, 1, 3)\n",
    "        \n",
    "        scores = attention(q, k, v, self.d_k, mask, dec_mask)\n",
    "        \n",
    "        concat = scores.transpose(0, 2, 1, 3).reshape([bs, -1, self.embedding_dim])\n",
    "        output = self.out(concat)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0a44b7-d771-44be-a18c-f1286f7820bc",
   "metadata": {},
   "source": [
    "### FeedForwardとLayer Norm\n",
    "\n",
    "Transformer Blockに存在するその他のモジュールを定義します．\n",
    "\n",
    "一つ目はFeedForward モジュールです．\n",
    "これは，通常の全結合層2層から構成されるネットワークのことを表しています．\n",
    "\n",
    "\n",
    "二つ目はLayer Normalizationです．\n",
    "Layer Normalizationについては，以前の演習で行っていますので，詳細はそちらを確認してください．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870786ca-e23d-42b4-89c2-e4c831af9571",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward:\n",
    "    def __init__(self, embedding_dim, d_ff=2048):\n",
    "        self.linear_1 = Linear(embedding_dim, d_ff)\n",
    "        self.linear_2 = Linear(d_ff, embedding_dim)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        h = relu(self.linear_1(x))\n",
    "        h = self.linear_2(h)\n",
    "        return h\n",
    "    \n",
    "class Norm:\n",
    "    def __init__(self, embedding_dim, eps=1e-6):\n",
    "        self.size = embedding_dim\n",
    "        self.alpha = np.ones(self.size)\n",
    "        self.bias = np.zeros(self.size)\n",
    "        \n",
    "        self.eps = eps\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        norm = self.alpha * (x - x.mean(axis=-1, keepdims=True)) \\\n",
    "        / (x.std(axis=-1, keepdims=True) + self.eps) + self.bias\n",
    "        return norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae8c375-7535-4b57-a340-95ed2a162afc",
   "metadata": {},
   "source": [
    "### Encoder・Decoderの定義\n",
    "\n",
    "上記で定義したモジュールを用いて，TransformerのEncoderとDecoderを定義します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec0b87a-a9aa-4a56-8ba0-07718fabd716",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer:\n",
    "    def __init__(self, embedding_dim, heads):\n",
    "        self.norm_1 = Norm(embedding_dim)\n",
    "        self.norm_2 = Norm(embedding_dim)\n",
    "        self.attn = MultiHeadAttention(heads, embedding_dim)\n",
    "        self.ff = FeedForward(embedding_dim)\n",
    "\n",
    "    def __call__(self, x, mask):\n",
    "        x2 = self.norm_1(x)\n",
    "        x = x + self.attn(x2,x2,x2,mask, dec_mask=False)\n",
    "        x2 = self.norm_2(x)\n",
    "        x = x + self.ff(x2)\n",
    "        return x\n",
    "        \n",
    "class DecoderLayer:\n",
    "    def __init__(self, embedding_dim, heads):\n",
    "        self.norm_1 = Norm(embedding_dim)\n",
    "        self.norm_2 = Norm(embedding_dim)\n",
    "        self.norm_3 = Norm(embedding_dim)\n",
    "        \n",
    "        self.attn_1 = MultiHeadAttention(heads, embedding_dim)\n",
    "        self.attn_2 = MultiHeadAttention(heads, embedding_dim)\n",
    "        self.ff = FeedForward(embedding_dim)\n",
    "\n",
    "    def __call__(self, x, e_outputs, src_mask, trg_mask):\n",
    "        x2 = self.norm_1(x)\n",
    "        x = x + self.attn_1(x2, x2, x2, trg_mask, dec_mask=True)\n",
    "        x2 = self.norm_2(x)\n",
    "        x = x + self.attn_2(x2, e_outputs, e_outputs, src_mask, dec_mask=False)\n",
    "        x2 = self.norm_3(x)\n",
    "        x = x + self.ff(x2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ca5fe3-5ae1-453d-b2c3-2eb2a08c3889",
   "metadata": {},
   "source": [
    "### Transformer\n",
    "\n",
    "最後にTransformer全体のネットワークを定義します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8faaf0-438c-489a-a11e-0d590c47bcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clones(module, N):\n",
    "    return [copy.deepcopy(module) for i in range(N)]\n",
    "\n",
    "class Encoder:\n",
    "    def __init__(self, vocab_size, embedding_dim, N, heads):\n",
    "        self.N = N\n",
    "        self.embed = Embedding(vocab_size, embedding_dim)\n",
    "        self.pe = PositionalEncoder(embedding_dim)\n",
    "        self.layers = get_clones(EncoderLayer(embedding_dim, heads), N)\n",
    "        self.norm = Norm(embedding_dim)\n",
    "        \n",
    "    def __call__(self, src, mask):\n",
    "        x = self.embed(src)\n",
    "        x = self.pe(x)\n",
    "        for i in range(self.N):\n",
    "            x = self.layers[i](x, mask)\n",
    "        return self.norm(x)\n",
    "    \n",
    "class Decoder:\n",
    "    def __init__(self, vocab_size, embedding_dim, N, heads):\n",
    "        self.N = N\n",
    "        self.embed = Embedding(vocab_size, embedding_dim)\n",
    "        self.pe = PositionalEncoder(embedding_dim)\n",
    "        self.layers = get_clones(DecoderLayer(embedding_dim, heads), N)\n",
    "        self.norm = Norm(embedding_dim)\n",
    "        \n",
    "    def __call__(self, trg, e_outputs, src_mask, trg_mask):\n",
    "        x = self.embed(trg)\n",
    "        x = self.pe(x)\n",
    "        for i in range(self.N):\n",
    "            x = self.layers[i](x, e_outputs, src_mask, trg_mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class Transformer:\n",
    "    def __init__(self, vocab_size, embedding_dim, N, heads):\n",
    "        self.encoder = Encoder(vocab_size, embedding_dim, N, heads)\n",
    "        self.decoder = Decoder(vocab_size, embedding_dim, N, heads)\n",
    "        self.out = Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def __call__(self, src, trg, src_mask, trg_mask):\n",
    "        e_outputs = self.encoder(src, src_mask)\n",
    "        d_output = self.decoder(trg, e_outputs, src_mask, trg_mask)\n",
    "        output = self.out(d_output)\n",
    "        return output\n",
    "\n",
    "def get_model(embedding_dim, heads, n_layers, vocab_size):\n",
    "    assert embedding_dim % heads == 0\n",
    "    model = Transformer(vocab_size, embedding_dim, n_layers, heads)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a0a8c9-6cb4-4153-84cb-2cb214580ea9",
   "metadata": {},
   "source": [
    "## Transformerの実行\n",
    "\n",
    "作成したTransformerモデルと計算機のデータセットを用いて，推論処理を行います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969bce89-0198-4eda-a8e1-8602a458dcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformerモデルの準備\n",
    "embedding_dim = 512\n",
    "n_layers = 6\n",
    "heads = 8\n",
    "vocab_size = len(word2id)\n",
    "\n",
    "model = get_model(embedding_dim, heads, n_layers, vocab_size)\n",
    "\n",
    "# データセットの準備\n",
    "batch_size = 1\n",
    "test_data = CalcDataset(data_num=10)\n",
    "\n",
    "\n",
    "accuracy = 0\n",
    "        \n",
    "# 評価の実行\n",
    "for i in range(len(test_data)):\n",
    "    \n",
    "    src, trg = test_data[i]\n",
    "    src = np.expand_dims(src, axis=0)\n",
    "    trg = np.expand_dims(trg, axis=0)\n",
    "\n",
    "    trg_input = trg[:, :-1].copy()\n",
    "    src_mask, trg_mask = create_masks(batch_size, src, trg_input)\n",
    "\n",
    "    # encoder\n",
    "    e_output = model.encoder(src, src_mask)\n",
    "\n",
    "    # decoder\n",
    "    right = []\n",
    "    for s in range(4):\n",
    "        outputs = trg_input[:, :s+1]\n",
    "        trg_mask_ = trg_mask[:, :s+1, :s+1]\n",
    "        out = model.out(model.decoder(outputs, e_output, src_mask, trg_mask_))\n",
    "        out = softmax(out, dim=2)\n",
    "\n",
    "        if s == 0:\n",
    "            index = np.argmax(out).item()\n",
    "        else:\n",
    "            index = np.argmax(out, axis=2)[0, -1]\n",
    "        token = id2word[index]\n",
    "\n",
    "        if token == \"<eos>\":\n",
    "            break\n",
    "        right.append(token)\n",
    "\n",
    "        trg_input[:, s+1] = float(word2id[token])\n",
    "    right = \"\".join(right)\n",
    "\n",
    "    x = list(src[0])\n",
    "    try:\n",
    "        padded_idx_x = x.index(word2id[\"<pad>\"])\n",
    "    except ValueError:\n",
    "        padded_idx_x = len(x)\n",
    "        \n",
    "    left = \"\".join(map(lambda c: str(id2word[c]), x[:padded_idx_x]))\n",
    "    flag = [\"F\", \"T\"][eval(left) == int(right)]\n",
    "    print(\"{:>7s} = {:>4s} :{}\".format(left, right, flag))\n",
    "    if flag == \"T\":\n",
    "        accuracy += 1\n",
    "\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy / len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2492f6-24fd-41dc-82b8-9e8337c6cd6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
